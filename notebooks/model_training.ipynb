{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```json\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Pixel Paladin RL - Model Training Notebook\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Description:** Train an AI agent using Reinforcement Learning (DQN) to master a custom-built Pygame maze environment. The agent learns optimal policies through trial-and-error interaction.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Setup and Imports\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import pygame\\n\",\n    \"import numpy as np\\n\",\n    \"import tensorflow as tf\\n\",\n    \"from tensorflow.keras import layers, models, optimizers, losses\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import random\\n\",\n    \"from collections import deque\\n\",\n    \"import time\\n\",\n    \"import gymnasium as gym\\n\",\n    \"from gymnasium import spaces\\n\",\n    \"import os\\n\",\n    \"\\n\",\n    \"# Suppress pygame welcome message\\n\",\n    \"os.environ['PYGAME_HIDE_SUPPORT_PROMPT'] = \\\"hide\\\"\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Custom Pygame Environment Definition\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class MazeEnv(gym.Env):\\n\",\n    \"    metadata = {'render_modes': ['human', 'rgb_array'], 'render_fps': 10}\\n\",\n    \"\\n\",\n    \"    def __init__(self, maze=None, render_mode=None, size=10):\\n\",\n    \"        super().__init__()\\n\",\n    \"\\n\",\n    \"        self.size = size  # Size of the maze grid (size x size)\\n\",\n    \"        self.cell_size = 40 # Pixel size of each grid cell\\n\",\n    \"        self.window_size = self.size * self.cell_size # Pygame window dimensions\\n\",\n    \"\\n\",\n    \"        # Define action and observation space\\n\",\n    \"        # Actions: 0=Up, 1=Down, 2=Left, 3=Right\\n\",\n    \"        self.action_space = spaces.Discrete(4)\\n\",\n    \"        # Observation is the agent's (row, col) position\\n\",\n    \"        self.observation_space = spaces.Box(0, self.size - 1, shape=(2,), dtype=int)\\n\",\n    \"\\n\",\n    \"        # Default maze if none provided\\n\",\n    \"        if maze is None:\\n\",\n    \"            self.maze = self._generate_default_maze()\\n\",\n    \"        else:\\n\",\n    \"            self.maze = np.array(maze)\\n\",\n    \"            assert self.maze.shape == (self.size, self.size), \\\"Provided maze has incorrect dimensions\\\"\\n\",\n    \"\\n\",\n    \"        # Find start and goal positions\\n\",\n    \"        start_pos = np.argwhere(self.maze == 2)\\n\",\n    \"        goal_pos = np.argwhere(self.maze == 3)\\n\",\n    \"        assert len(start_pos) == 1, \\\"Maze must have exactly one start point (marked with 2)\\\"\\n\",\n    \"        assert len(goal_pos) == 1, \\\"Maze must have exactly one goal point (marked with 3)\\\"\\n\",\n    \"        self._start_pos = tuple(start_pos[0])\\n\",\n    \"        self._goal_pos = tuple(goal_pos[0])\\n\",\n    \"\\n\",\n    \"        self._agent_location = None\\n\",\n    \"\\n\",\n    \"        assert render_mode is None or render_mode in self.metadata[\\\"render_modes\\\"]\\n\",\n    \"        self.render_mode = render_mode\\n\",\n    \"\\n\",\n    \"        self.window = None\\n\",\n    \"        self.clock = None\\n\",\n    \"\\n\",\n    \"    def _generate_default_maze(self):\\n\",\n    \"        # 0: Empty, 1: Wall, 2: Start, 3: Goal\\n\",\n    \"        maze = np.zeros((self.size, self.size), dtype=int)\\n\",\n    \"        # Add boundary walls\\n\",\n    \"        maze[0, :] = 1\\n\",\n    \"        maze[-1, :] = 1\\n\",\n    \"        maze[:, 0] = 1\\n\",\n    \"        maze[:, -1] = 1\\n\",\n    \"        # Add some internal walls\\n\",\n    \"        maze[1:self.size-1, self.size // 2] = 1\\n\",\n    \"        maze[self.size // 2, 1:self.size-1] = 1\\n\",\n    \"        maze[self.size // 2, self.size // 2] = 0 # Clear center crossing\\n\",\n    \"        # Start and Goal\\n\",\n    \"        maze[1, 1] = 2\\n\",\n    \"        maze[self.size - 2, self.size - 2] = 3\\n\",\n    \"        return maze\\n\",\n    \"\\n\",\n    \"    def _get_obs(self):\\n\",\n    \"        return np.array(self._agent_location, dtype=int)\\n\",\n    \"\\n\",\n    \"    def _get_info(self):\\n\",\n    \"        # Optional info, e.g., distance to goal (not used by agent directly)\\n\",\n    \"        return {\\\"distance\\\": np.linalg.norm(np.array(self._agent_location) - np.array(self._goal_pos))}\\n\",\n    \"\\n\",\n    \"    def reset(self, seed=None, options=None):\\n\",\n    \"        super().reset(seed=seed)\\n\",\n    \"        self._agent_location = self._start_pos\\n\",\n    \"        observation = self._get_obs()\\n\",\n    \"        info = self._get_info()\\n\",\n    \"\\n\",\n    \"        if self.render_mode == \\\"human\\\":\\n\",\n    \"            self._render_frame()\\n\",\n    \"\\n\",\n    \"        return observation, info\\n\",\n    \"\\n\",\n    \"    def step(self, action):\\n\",\n    \"        # Map the action (element of {0, 1, 2, 3}) to the direction we walk in\\n\",\n    \"        direction_map = {\\n\",\n    \"            0: (-1, 0),  # Up\\n\",\n    \"            1: (1, 0),   # Down\\n\",\n    \"            2: (0, -1),  # Left\\n\",\n    \"            3: (0, 1)    # Right\\n\",\n    \"        }\\n\",\n    \"        direction = direction_map[action]\\n\",\n    \"\\n\",\n    \"        # Calculate potential new position\\n\",\n    \"        new_location = (\\n\",\n    \"            self._agent_location[0] + direction[0],\\n\",\n    \"            self._agent_location[1] + direction[1]\\n\",\n    \"        )\\n\",\n    \"\\n\",\n    \"        terminated = False\\n\",\n    \"        reward = -0.1 # Small penalty for each step to encourage efficiency\\n\",\n    \"\\n\",\n    \"        # Check if the new location is valid (within bounds and not a wall)\\n\",\n    \"        if (0 <= new_location[0] < self.size and\\n\",\n    \"            0 <= new_location[1] < self.size and\\n\",\n    \"            self.maze[new_location[0], new_location[1]] != 1):\\n\",\n    \"            self._agent_location = new_location\\n\",\n    \"        else:\\n\",\n    \"            reward = -0.5 # Penalty for hitting a wall\\n\",\n    \"\\n\",\n    \"        # Check if the agent reached the goal\\n\",\n    \"        if self._agent_location == self._goal_pos:\\n\",\n    \"            reward = 10.0 # Large reward for reaching the goal\\n\",\n    \"            terminated = True\\n\",\n    \"\\n\",\n    \"        truncated = False # We don't have a step limit here, but could add one\\n\",\n    \"        observation = self._get_obs()\\n\",\n    \"        info = self._get_info()\\n\",\n    \"\\n\",\n    \"        if self.render_mode == \\\"human\\\":\\n\",\n    \"            self._render_frame()\\n\",\n    \"\\n\",\n    \"        return observation, reward, terminated, truncated, info\\n\",\n    \"\\n\",\n    \"    def render(self):\\n\",\n    \"        if self.render_mode == \\\"rgb_array\\\":\\n\",\n    \"            return self._render_frame()\\n\",\n    \"        elif self.render_mode == \\\"human\\\":\\n\",\n    \"             self._render_frame()\\n\",\n    \"\\n\",\n    \"    def _render_frame(self):\\n\",\n    \"        if self.window is None and self.render_mode == \\\"human\\\":\\n\",\n    \"            pygame.init()\\n\",\n    \"            pygame.display.init()\\n\",\n    \"            self.window = pygame.display.set_mode((self.window_size, self.window_size))\\n\",\n    \"            pygame.display.set_caption(\\\"Pixel Paladin RL - Maze Environment\\\")\\n\",\n    \"        if self.clock is None and self.render_mode == \\\"human\\\":\\n\",\n    \"            self.clock = pygame.time.Clock()\\n\",\n    \"\\n\",\n    \"        canvas = pygame.Surface((self.window_size, self.window_size))\\n\",\n    \"        canvas.fill((255, 255, 255)) # White background\\n\",\n    \"\\n\",\n    \"        # Draw the maze elements\\n\",\n    \"        for r in range(self.size):\\n\",\n    \"            for c in range(self.size):\\n\",\n    \"                rect = pygame.Rect(c * self.cell_size, r * self.cell_size, self.cell_size, self.cell_size)\\n\",\n    \"                if self.maze[r, c] == 1: # Wall\\n\",\n    \"                    pygame.draw.rect(canvas, (0, 0, 0), rect) # Black\\n\",\n    \"                elif self.maze[r, c] == 3: # Goal\\n\",\n    \"                    pygame.draw.rect(canvas, (0, 255, 0), rect) # Green\\n\",\n    \"                elif (r, c) == self._start_pos: # Start (only draw if agent isn't there)\\n\",\n    \"                     if self._agent_location != self._start_pos:\\n\",\n    \"                         pygame.draw.rect(canvas, (200, 200, 200), rect) # Light Gray\\n\",\n    \"\\n\",\n    \"        # Draw the agent\\n\",\n    \"        agent_rect = pygame.Rect(\\n\",\n    \"            self._agent_location[1] * self.cell_size + self.cell_size // 4,\\n\",\n    \"            self._agent_location[0] * self.cell_size + self.cell_size // 4,\\n\",\n    \"            self.cell_size // 2,\\n\",\n    \"            self.cell_size // 2\\n\",\n    \"        )\\n\",\n    \"        pygame.draw.rect(canvas, (0, 0, 255), agent_rect) # Blue\\n\",\n    \"\\n\",\n    \"        if self.render_mode == \\\"human\\\":\\n\",\n    \"            # Update the screen\\n\",\n    \"            self.window.blit(canvas, canvas.get_rect())\\n\",\n    \"            pygame.event.pump()\\n\",\n    \"            pygame.display.update()\\n\",\n    \"            self.clock.tick(self.metadata[\\\"render_fps\\\"])\\n\",\n    \"        else:  # rgb_array\\n\",\n    \"            return np.transpose(\\n\",\n    \"                np.array(pygame.surfarray.pixels3d(canvas)), axes=(1, 0, 2)\\n\",\n    \"            )\\n\",\n    \"\\n\",\n    \"    def close(self):\\n\",\n    \"        if self.window is not None:\\n\",\n    \"            pygame.display.quit()\\n\",\n    \"            pygame.quit()\\n\",\n    \"            self.window = None\\n\",\n    \"            self.clock = None\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Hyperparameter Definition\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Environment parameters\\n\",\n    \"MAZE_SIZE = 10\\n\",\n    \"\\n\",\n    \"# DQN Agent parameters\\n\",\n    \"STATE_SIZE = 2 # (row, col)\\n\",\n    \"ACTION_SIZE = 4 # (up, down, left, right)\\n\",\n    \"LEARNING_RATE = 0.001\\n\",\n    \"GAMMA = 0.99         # Discount factor for future rewards\\n\",\n    \"MEMORY_SIZE = 10000  # Max size of the replay buffer\\n\",\n    \"BATCH_SIZE = 64      # Number of experiences to sample from memory for training\\n\",\n    \"\\n\",\n    \"# Exploration parameters (Epsilon-Greedy)\\n\",\n    \"EPSILON_START = 1.0\\n\",\n    \"EPSILON_END = 0.01\\n\",\n    \"EPSILON_DECAY_STEPS = 10000 # How many steps to decay epsilon over\\n\",\n    \"\\n\",\n    \"# Training parameters\\n\",\n    \"TOTAL_EPISODES = 500\\n\",\n    \"MAX_STEPS_PER_EPISODE = 200 # Prevent infinitely running episodes\\n\",\n    \"TARGET_UPDATE_FREQ = 100   # How often (in steps) to update the target network\\n\",\n    \"TRAIN_START_STEPS = 1000   # Start training only after this many steps have been collected\\n\",\n    \"LEARNING_FREQ = 4          # Train the model every N steps\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. DQN Agent Definition\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"class DQNAgent:\\n\",\n    \"    def __init__(self, state_size, action_size, learning_rate, gamma, memory_size, batch_size,\\n\",\n    \"                 epsilon_start, epsilon_end, epsilon_decay_steps):\\n\",\n    \"        self.state_size = state_size\\n\",\n    \"        self.action_size = action_size\\n\",\n    \"        self.memory = deque(maxlen=memory_size)\\n\",\n    \"        self.gamma = gamma\\n\",\n    \"        self.epsilon = epsilon_start\\n\",\n    \"        self.epsilon_start = epsilon_start\\n\",\n    \"        self.epsilon_end = epsilon_end\\n\",\n    \"        self.epsilon_decay_steps = epsilon_decay_steps\\n\",\n    \"        self.batch_size = batch_size\\n\",\n    \"        self.learning_rate = learning_rate\\n\",\n    \"        self.model = self._build_model()\\n\",\n    \"        self.target_model = self._build_model()\\n\",\n    \"        self.update_target_model() # Initialize target model weights\\n\",\n    \"        self.optimizer = optimizers.Adam(learning_rate=self.learning_rate)\\n\",\n    \"        self.loss_function = losses.MeanSquaredError()\\n\",\n    \"        self.step_count = 0\\n\",\n    \"\\n\",\n    \"    def _build_model(self):\\n\",\n    \"        # Simple Feed Forward Neural Network\\n\",\n    \"        model = models.Sequential([\\n\",\n    \"            layers.Input(shape=(self.state_size,)),\\n\",\n    \"            layers.Dense(64, activation='relu'),\\n\",\n    \"            layers.Dense(64, activation='relu'),\\n\",\n    \"            layers.Dense(self.action_size, activation='linear') # Q-values for each action\\n\",\n    \"        ])\\n\",\n    \"        # No compile here, we handle loss and optimization manually in train_step\\n\",\n    \"        return model\\n\",\n    \"\\n\",\n    \"    def update_target_model(self):\\n\",\n    \"        # Copy weights from model to target_model\\n\",\n    \"        self.target_model.set_weights(self.model.get_weights())\\n\",\n    \"\\n\",\n    \"    def remember(self, state, action, reward, next_state, done):\\n\",\n    \"        # Store experience tuple in replay memory\\n\",\n    \"        self.memory.append((state, action, reward, next_state, done))\\n\",\n    \"\\n\",\n    \"    def act(self, state):\\n\",\n    \"        # Epsilon-greedy action selection\\n\",\n    \"        self.step_count += 1\\n\",\n    \"        # Update epsilon\\n\",\n    \"        epsilon_decay = (self.epsilon_start - self.epsilon_end) / self.epsilon_decay_steps\\n\",\n    \"        self.epsilon = max(self.epsilon_end, self.epsilon_start - epsilon_decay * self.step_count)\\n\",\n    \"\\n\",\n    \"        if np.random.rand() <= self.epsilon:\\n\",\n    \"            return random.randrange(self.action_size) # Explore: random action\\n\",\n    \"        else:\\n\",\n    \"            # Exploit: predict Q-values and choose the best action\\n\",\n    \"            state_tensor = tf.convert_to_tensor(state)\\n\",\n    \"            state_tensor = tf.expand_dims(state_tensor, 0) # Add batch dimension\\n\",\n    \"            act_values = self.model(state_tensor, training=False)\\n\",\n    \"            return np.argmax(act_values[0].numpy()) # Choose action with highest Q-value\\n\",\n    \"\\n\",\n    \"    @tf.function # Decorator for potential performance improvement\\n\",\n    \"    def train_step(self, states, actions, rewards, next_states, dones):\\n\",\n    \"        # Predict Q-values for the next states using the target network\\n\",\n    \"        future_rewards = self.target_model(next_states, training=False)\\n\",\n    \"        # Q(s', a') = max_a' Q_target(s', a')\\n\",\n    \"        updated_q_values = rewards + self.gamma * tf.reduce_max(future_rewards, axis=1) * (1 - dones)\\n\",\n    \"\\n\",\n    \"        # Create a mask to only update the Q-value for the action taken\\n\",\n    \"        masks = tf.one_hot(actions, self.action_size)\\n\",\n    \"\\n\",\n    \"        with tf.GradientTape() as tape:\\n\",\n    \"            # Predict Q-values for the current states using the main network\\n\",\n    \"            q_values = self.model(states, training=True)\\n\",\n    \"            # Select the Q-value for the action that was actually taken\\n\",\n    \"            q_action = tf.reduce_sum(tf.multiply(q_values, masks), axis=1)\\n\",\n    \"            # Calculate loss between predicted Q-value and target Q-value\\n\",\n    \"            loss = self.loss_function(updated_q_values, q_action)\\n\",\n    \"\\n\",\n    \"        # Calculate gradients and update the main network weights\\n\",\n    \"        grads = tape.gradient(loss, self.model.trainable_variables)\\n\",\n    \"        self.optimizer.apply_gradients(zip(grads, self.model.trainable_variables))\\n\",\n    \"        return loss\\n\",\n    \"\\n\",\n    \"    def replay(self):\\n\",\n    \"        # Train the network using experiences sampled from memory\\n\",\n    \"        if len(self.memory) < self.batch_size:\\n\",\n    \"            return 0 # Not enough memory yet\\n\",\n    \"\\n\",\n    \"        # Sample a minibatch of experiences\\n\",\n    \"        minibatch = random.sample(self.memory, self.batch_size)\\n\",\n    \"\\n\",\n    \"        # Separate the components of the minibatch\\n\",\n    \"        states = np.array([experience[0] for experience in minibatch])\\n\",\n    \"        actions = np.array([experience[1] for experience in minibatch])\\n\",\n    \"        rewards = np.array([experience[2] for experience in minibatch])\\n\",\n    \"        next_states = np.array([experience[3] for experience in minibatch])\\n\",\n    \"        dones = np.array([experience[4] for experience in minibatch]).astype(np.float32) # Convert boolean to float for calculation\\n\",\n    \"\\n\",\n    \"        # Convert numpy arrays to TensorFlow tensors\\n\",\n    \"        states_tensor = tf.convert_to_tensor(states, dtype=tf.float32)\\n\",\n    \"        actions_tensor = tf.convert_to_tensor(actions, dtype=tf.int32)\\n\",\n    \"        rewards_tensor = tf.convert_to_tensor(rewards, dtype=tf.float32)\\n\",\n    \"        next_states_tensor = tf.convert_to_tensor(next_states, dtype=tf.float32)\\n\",\n    \"        dones_tensor = tf.convert_to_tensor(dones, dtype=tf.float32)\\n\",\n    \"\\n\",\n    \"        loss = self.train_step(states_tensor, actions_tensor, rewards_tensor, next_states_tensor, dones_tensor)\\n\",\n    \"        return loss.numpy()\\n\",\n    \"\\n\",\n    \"    def load(self, name):\\n\",\n    \"        self.model.load_weights(name)\\n\",\n    \"        self.update_target_model() # Ensure target model is also updated\\n\",\n    \"\\n\",\n    \"    def save(self, name):\\n\",\n    \"        self.model.save_weights(name)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 5. Model Training\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"# Initialize environment and agent\\n\",\n    \"env = MazeEnv(size=MAZE_SIZE)\\n\",\n    \"agent = DQNAgent(\\n\",\n    \"    state_size=STATE_SIZE,\\n\",\n    \"    action_size=ACTION_SIZE,\\n\",\n    \"    learning_rate=LEARNING_RATE,\\n\",\n    \"    gamma=GAMMA,\\n\",\n    \"    memory_size=MEMORY_SIZE,\\n\",\n    \"    batch_size=BATCH_SIZE,\\n\",\n    \"    epsilon_start=EPSILON_START,\\n\",\n    \"    epsilon_end=EPSILON_END,\\n\",\n    \"    epsilon_decay_steps=EPSILON_DECAY_STEPS\\n\",\n    \")\\n\",\n    \"\\n\",\n    \"episode_rewards = []\\n\",\n    \"training_losses = []\\n\",\n    \"total_steps = 0\\n\",\n    \"start_time = time.time()\\n\",\n    \"\\n\",\n    \"print(f\\\"Starting training for {TOTAL_EPISODES} episodes...\\\")\\n\",\n    \"\\n\",\n    \"for episode in range(1, TOTAL_EPISODES + 1):\\n\",\n    \"    state, _ = env.reset()\\n\",\n    \"    state = np.reshape(state, [1, STATE_SIZE]).astype(np.float32) # Reshape and ensure float type\\n\",\n    \"    episode_reward = 0\\n\",\n    \"    episode_loss = []\\n\",\n    \"\\n\",\n    \"    for step in range(1, MAX_STEPS_PER_EPISODE + 1):\\n\",\n    \"        # Select action\\n\",\n    \"        action = agent.act(state)\\n\",\n    \"\\n\",\n    \"        # Take action in environment\\n\",\n    \"        next_state, reward, terminated, truncated, _ = env.step(action)\\n\",\n    \"        next_state = np.reshape(next_state, [1, STATE_SIZE]).astype(np.float32)\\n\",\n    \"        done = terminated or truncated\\n\",\n    \"\\n\",\n    \"        # Store experience\\n\",\n    \"        agent.remember(state[0], action, reward, next_state[0], done) # Store flattened state\\n\",\n    \"\\n\",\n    \"        # Move to next state\\n\",\n    \"        state = next_state\\n\",\n    \"        episode_reward += reward\\n\",\n    \"        total_steps += 1\\n\",\n    \"\\n\",\n    \"        # Train the agent\\n\",\n    \"        if total_steps > TRAIN_START_STEPS and total_steps % LEARNING_FREQ == 0:\\n\",\n    \"            loss = agent.replay()\\n\",\n    \"            if loss is not None:\\n\",\n    \"                 episode_loss.append(loss)\\n\",\n    \"\\n\",\n    \"        # Update target network\\n\",\n    \"        if total_steps % TARGET_UPDATE_FREQ == 0:\\n\",\n    \"            agent.update_target_model()\\n\",\n    \"\\n\",\n    \"        if"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}