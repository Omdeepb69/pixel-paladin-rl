{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "```json\n{\n \"cells\": [\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"# Pixel Paladin RL: Data Exploration\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"**Description:** 'Train an AI agent using Reinforcement Learning to master a custom-built Pygame environment, learning complex strategies beyond simple scripted behavior. It's dangerous to go alone; train this!'.\\n\",\n    \"\\n\",\n    \"**Features:** Custom Pygame 2D environment (e.g., maze navigation, simple platformer, or top-down shooter)., Implementation of a core Reinforcement Learning algorithm (like Q-Learning or basic DQN)., Agent learns optimal policies through trial-and-error interaction with the game., Visualization of the agent's learning progress (e.g., rewards per episode)., The agent exhibits emergent behaviors not explicitly programmed..\\n\",\n    \"\\n\",\n    \"**Objective:** Explore simulated training data (logs) from RL training sessions.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 1. Setup and Data Loading\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"import pandas as pd\\n\",\n    \"import numpy as np\\n\",\n    \"import matplotlib.pyplot as plt\\n\",\n    \"import seaborn as sns\\n\",\n    \"import json\\n\",\n    \"import os\\n\",\n    \"from scipy import stats\\n\",\n    \"\\n\",\n    \"# Set plot style\\n\",\n    \"sns.set_theme(style=\\\"whitegrid\\\")\\n\",\n    \"plt.rcParams['figure.figsize'] = (12, 6)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Generate Simulated Training Data\\n\",\n    \"\\n\",\n    \"Since we don't have actual logs yet, we'll simulate data representing a typical RL training run (e.g., DQN). This data will include episode number, reward, steps taken, loss, and exploration rate (epsilon).\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"def generate_simulated_data(num_episodes=1000, random_seed=42):\\n\",\n    \"    np.random.seed(random_seed)\\n\",\n    \"    \\n\",\n    \"    episodes = np.arange(1, num_episodes + 1)\\n\",\n    \"    \\n\",\n    \"    # Simulate Reward: Starts low, increases, then plateaus with noise\\n\",\n    \"    base_reward = -500 + 700 * (1 - np.exp(-episodes / 200))\\n\",\n    \"    noise_reward = np.random.normal(0, 50, num_episodes)\\n\",\n    \"    # Add some occasional dips/spikes\\n\",\n    \"    for _ in range(num_episodes // 50):\\n\",\n    \"        idx = np.random.randint(0, num_episodes)\\n\",\n    \"        noise_reward[idx] *= np.random.uniform(1.5, 3.0) * np.random.choice([-1, 1])\\n\",\n    \"    reward = base_reward + noise_reward\\n\",\n    \"    \\n\",\n    \"    # Simulate Steps: Starts high, decreases as agent learns, might slightly increase later\\n\",\n    \"    base_steps = 1000 * np.exp(-episodes / 300) + 50\\n\",\n    \"    noise_steps = np.random.normal(0, 20, num_episodes)\\n\",\n    \"    steps = np.maximum(10, base_steps + noise_steps).astype(int) # Ensure steps are positive\\n\",\n    \"    \\n\",\n    \"    # Simulate Loss (e.g., DQN loss): Starts high, decreases with noise\\n\",\n    \"    base_loss = 10 * np.exp(-episodes / 150) + 0.1\\n\",\n    \"    noise_loss = np.random.lognormal(0, 0.5, num_episodes) * 0.1 # Log-normal noise often seen in loss\\n\",\n    \"    loss = np.maximum(0.01, base_loss + noise_loss)\\n\",\n    \"    \\n\",\n    \"    # Simulate Epsilon (Exponential Decay)\\n\",\n    \"    epsilon_start = 1.0\\n\",\n    \"    epsilon_end = 0.05\\n\",\n    \"    epsilon_decay = 0.995\\n\",\n    \"    epsilon = []\\n\",\n    \"    current_epsilon = epsilon_start\\n\",\n    \"    for _ in range(num_episodes):\\n\",\n    \"        epsilon.append(current_epsilon)\\n\",\n    \"        current_epsilon = max(epsilon_end, current_epsilon * epsilon_decay)\\n\",\n    \"    epsilon = np.array(epsilon)\\n\",\n    \"    \\n\",\n    \"    df = pd.DataFrame({\\n\",\n    \"        'episode': episodes,\\n\",\n    \"        'reward': reward,\\n\",\n    \"        'steps': steps,\\n\",\n    \"        'loss': loss,\\n\",\n    \"        'epsilon': epsilon\\n\",\n    \"    })\\n\",\n    \"    \\n\",\n    \"    # Simulate a second run with slightly different parameters (e.g., learning rate)\\n\",\n    \"    np.random.seed(random_seed + 1) # Change seed for variation\\n\",\n    \"    base_reward_2 = -600 + 750 * (1 - np.exp(-episodes / 250)) # Slower initial learning, higher plateau\\n\",\n    \"    noise_reward_2 = np.random.normal(0, 60, num_episodes)\\n\",\n    \"    reward_2 = base_reward_2 + noise_reward_2\\n\",\n    \"    \\n\",\n    \"    base_steps_2 = 1100 * np.exp(-episodes / 350) + 60\\n\",\n    \"    noise_steps_2 = np.random.normal(0, 25, num_episodes)\\n\",\n    \"    steps_2 = np.maximum(15, base_steps_2 + noise_steps_2).astype(int)\\n\",\n    \"    \\n\",\n    \"    base_loss_2 = 12 * np.exp(-episodes / 180) + 0.15\\n\",\n    \"    noise_loss_2 = np.random.lognormal(0, 0.55, num_episodes) * 0.12\\n\",\n    \"    loss_2 = np.maximum(0.015, base_loss_2 + noise_loss_2)\\n\",\n    \"    \\n\",\n    \"    df_run2 = pd.DataFrame({\\n\",\n    \"        'episode': episodes,\\n\",\n    \"        'reward': reward_2,\\n\",\n    \"        'steps': steps_2,\\n\",\n    \"        'loss': loss_2,\\n\",\n    \"        'epsilon': epsilon # Assuming same epsilon schedule for comparison\\n\",\n    \"    })\\n\",\n    \"    \\n\",\n    \"    # Save to CSV\\n\",\n    \"    if not os.path.exists('data'):\\n\",\n    \"        os.makedirs('data')\\n\",\n    \"    df.to_csv('data/training_log_run1.csv', index=False)\\n\",\n    \"    df_run2.to_csv('data/training_log_run2.csv', index=False)\\n\",\n    \"    \\n\",\n    \"    print(\\\"Simulated data generated and saved to 'data/training_log_run1.csv' and 'data/training_log_run2.csv'\\\")\\n\",\n    \"    return df, df_run2\\n\",\n    \"\\n\",\n    \"df_run1, df_run2 = generate_simulated_data()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Load Data\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"try:\\n\",\n    \"    df_run1 = pd.read_csv('data/training_log_run1.csv')\\n\",\n    \"    df_run2 = pd.read_csv('data/training_log_run2.csv')\\n\",\n    \"    print(\\\"Training logs loaded successfully.\\\")\\n\",\n    \"except FileNotFoundError:\\n\",\n    \"    print(\\\"Error: Training log files not found. Please ensure they are in the 'data' directory.\\\")\\n\",\n    \"    # In a real scenario, you might stop execution or handle this differently\\n\",\n    \"    # For this example, we'll proceed with the data generated in the previous step if loading fails\\n\",\n    \"    if 'df_run1' not in locals():\\n\",\n    \"        print(\\\"Generating fresh simulated data as files were not found.\\\")\\n\",\n    \"        df_run1, df_run2 = generate_simulated_data()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Inspect Data\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"--- Run 1: First 5 Rows ---\\\")\\n\",\n    \"print(df_run1.head())\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 1: Data Info ---\\\")\\n\",\n    \"df_run1.info()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 1: Descriptive Statistics ---\\\")\\n\",\n    \"print(df_run1.describe())\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 1: Missing Values ---\\\")\\n\",\n    \"print(df_run1.isnull().sum())\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"--- Run 2: First 5 Rows ---\\\")\\n\",\n    \"print(df_run2.head())\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 2: Data Info ---\\\")\\n\",\n    \"df_run2.info()\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 2: Descriptive Statistics ---\\\")\\n\",\n    \"print(df_run2.describe())\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 2: Missing Values ---\\\")\\n\",\n    \"print(df_run2.isnull().sum())\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 2. Exploratory Data Analysis (EDA)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Learning Curves (Reward per Episode)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='reward', label='Run 1 Reward', alpha=0.7)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='reward', label='Run 2 Reward', alpha=0.7)\\n\",\n    \"\\n\",\n    \"# Add rolling average for smoother trend\\n\",\n    \"rolling_window = 50\\n\",\n    \"df_run1['reward_rolling'] = df_run1['reward'].rolling(window=rolling_window).mean()\\n\",\n    \"df_run2['reward_rolling'] = df_run2['reward'].rolling(window=rolling_window).mean()\\n\",\n    \"\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='reward_rolling', label=f'Run 1 Reward (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='reward_rolling', label=f'Run 2 Reward (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"\\n\",\n    \"plt.title('Reward per Episode Over Training')\\n\",\n    \"plt.xlabel('Episode')\\n\",\n    \"plt.ylabel('Total Reward')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.grid(True)\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Steps per Episode\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='steps', label='Run 1 Steps', alpha=0.7)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='steps', label='Run 2 Steps', alpha=0.7)\\n\",\n    \"\\n\",\n    \"# Add rolling average\\n\",\n    \"df_run1['steps_rolling'] = df_run1['steps'].rolling(window=rolling_window).mean()\\n\",\n    \"df_run2['steps_rolling'] = df_run2['steps'].rolling(window=rolling_window).mean()\\n\",\n    \"\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='steps_rolling', label=f'Run 1 Steps (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='steps_rolling', label=f'Run 2 Steps (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"\\n\",\n    \"plt.title('Steps per Episode Over Training')\\n\",\n    \"plt.xlabel('Episode')\\n\",\n    \"plt.ylabel('Steps Taken')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.grid(True)\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Loss Curve (if applicable, e.g., for DQN)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='loss', label='Run 1 Loss', alpha=0.7)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='loss', label='Run 2 Loss', alpha=0.7)\\n\",\n    \"\\n\",\n    \"# Add rolling average\\n\",\n    \"df_run1['loss_rolling'] = df_run1['loss'].rolling(window=rolling_window).mean()\\n\",\n    \"df_run2['loss_rolling'] = df_run2['loss'].rolling(window=rolling_window).mean()\\n\",\n    \"\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='loss_rolling', label=f'Run 1 Loss (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='loss_rolling', label=f'Run 2 Loss (Rolling Avg {rolling_window})', linewidth=2)\\n\",\n    \"\\n\",\n    \"plt.title('Training Loss per Episode')\\n\",\n    \"plt.xlabel('Episode')\\n\",\n    \"plt.ylabel('Loss')\\n\",\n    \"plt.yscale('log') # Loss often benefits from a log scale\\n\",\n    \"plt.legend()\\n\",\n    \"plt.grid(True, which=\\\"both\\\", ls=\\\"--\\\")\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Epsilon Decay\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='epsilon', label='Epsilon Decay')\\n\",\n    \"plt.title('Exploration Rate (Epsilon) Over Training')\\n\",\n    \"plt.xlabel('Episode')\\n\",\n    \"plt.ylabel('Epsilon Value')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.grid(True)\\n\",\n    \"plt.show()\\n\",\n    \"# Note: Epsilon is the same for both runs in this simulation, so only one line is plotted.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Distribution of Rewards (Overall)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.histplot(df_run1['reward'], kde=True, label='Run 1 Reward Distribution', color='skyblue', bins=50)\\n\",\n    \"sns.histplot(df_run2['reward'], kde=True, label='Run 2 Reward Distribution', color='lightcoral', bins=50)\\n\",\n    \"plt.title('Overall Distribution of Rewards per Episode')\\n\",\n    \"plt.xlabel('Total Reward')\\n\",\n    \"plt.ylabel('Frequency')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Distribution of Rewards (Late Training Phase)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"late_phase_start_episode = int(len(df_run1) * 0.8) # Look at the last 20% of episodes\\n\",\n    \"df_run1_late = df_run1[df_run1['episode'] >= late_phase_start_episode]\\n\",\n    \"df_run2_late = df_run2[df_run2['episode'] >= late_phase_start_episode]\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.histplot(df_run1_late['reward'], kde=True, label='Run 1 Reward (Late Phase)', color='skyblue', bins=30)\\n\",\n    \"sns.histplot(df_run2_late['reward'], kde=True, label='Run 2 Reward (Late Phase)', color='lightcoral', bins=30)\\n\",\n    \"plt.title(f'Distribution of Rewards per Episode (Episodes >= {late_phase_start_episode})')\\n\",\n    \"plt.xlabel('Total Reward')\\n\",\n    \"plt.ylabel('Frequency')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Correlation Analysis\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"correlation_matrix_run1 = df_run1[['reward', 'steps', 'loss', 'epsilon']].corr()\\n\",\n    \"correlation_matrix_run2 = df_run2[['reward', 'steps', 'loss', 'epsilon']].corr()\\n\",\n    \"\\n\",\n    \"fig, axes = plt.subplots(1, 2, figsize=(16, 6))\\n\",\n    \"\\n\",\n    \"sns.heatmap(correlation_matrix_run1, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[0])\\n\",\n    \"axes[0].set_title('Run 1: Correlation Matrix')\\n\",\n    \"\\n\",\n    \"sns.heatmap(correlation_matrix_run2, annot=True, cmap='coolwarm', fmt='.2f', ax=axes[1])\\n\",\n    \"axes[1].set_title('Run 2: Correlation Matrix')\\n\",\n    \"\\n\",\n    \"plt.tight_layout()\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 3. Statistical Analysis\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Summary Statistics for Key Metrics\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(\\\"--- Run 1: Summary Statistics ---\\\")\\n\",\n    \"print(df_run1[['reward', 'steps', 'loss']].agg(['mean', 'median', 'std', 'min', 'max']))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 2: Summary Statistics ---\\\")\\n\",\n    \"print(df_run2[['reward', 'steps', 'loss']].agg(['mean', 'median', 'std', 'min', 'max']))\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Performance Comparison (Late Training Phase)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"print(f\\\"--- Comparison of Late Phase (Episodes >= {late_phase_start_episode}) ---\\\")\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 1 (Late Phase) ---\\\")\\n\",\n    \"print(df_run1_late[['reward', 'steps', 'loss']].agg(['mean', 'median', 'std']))\\n\",\n    \"\\n\",\n    \"print(\\\"\\\\n--- Run 2 (Late Phase) ---\\\")\\n\",\n    \"print(df_run2_late[['reward', 'steps', 'loss']].agg(['mean', 'median', 'std']))\\n\",\n    \"\\n\",\n    \"# Perform t-test to compare mean rewards in the late phase\\n\",\n    \"t_stat, p_value = stats.ttest_ind(df_run1_late['reward'], df_run2_late['reward'], equal_var=False) # Welch's t-test\\n\",\n    \"\\n\",\n    \"print(f\\\"\\\\n--- T-test for Mean Reward in Late Phase ---\\\")\\n\",\n    \"print(f\\\"T-statistic: {t_stat:.4f}\\\")\\n\",\n    \"print(f\\\"P-value: {p_value:.4f}\\\")\\n\",\n    \"\\n\",\n    \"alpha = 0.05\\n\",\n    \"if p_value < alpha:\\n\",\n    \"    print(f\\\"The difference in mean rewards between Run 1 and Run 2 during the late phase is statistically significant (p < {alpha}).\\\")\\n\",\n    \"else:\\n\",\n    \"    print(f\\\"There is no statistically significant difference in mean rewards between Run 1 and Run 2 during the late phase (p >= {alpha}).\\\")\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Learning Stability (Rolling Standard Deviation)\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"rolling_std_window = 50\\n\",\n    \"df_run1['reward_rolling_std'] = df_run1['reward'].rolling(window=rolling_std_window).std()\\n\",\n    \"df_run2['reward_rolling_std'] = df_run2['reward'].rolling(window=rolling_std_window).std()\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='reward_rolling_std', label=f'Run 1 Reward Rolling Std Dev ({rolling_std_window})')\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='reward_rolling_std', label=f'Run 2 Reward Rolling Std Dev ({rolling_std_window})')\\n\",\n    \"\\n\",\n    \"plt.title('Reward Stability (Rolling Standard Deviation)')\\n\",\n    \"plt.xlabel('Episode')\\n\",\n    \"plt.ylabel(f'Rolling Std Dev (Window={rolling_std_window})')\\n\",\n    \"plt.legend()\\n\",\n    \"plt.grid(True)\\n\",\n    \"plt.show()\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"## 4. Feature Engineering Experiments (on Log Data)\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"We already created rolling averages (`reward_rolling`, `steps_rolling`, `loss_rolling`) and rolling standard deviation (`reward_rolling_std`) during EDA. Let's add cumulative reward and a simple success rate metric.\"\n   ]\n  },\n  {\n   \"cell_type\": \"markdown\",\n   \"metadata\": {},\n   \"source\": [\n    \"### Cumulative Reward\"\n   ]\n  },\n  {\n   \"cell_type\": \"code\",\n   \"execution_count\": null,\n   \"metadata\": {},\n   \"outputs\": [],\n   \"source\": [\n    \"df_run1['cumulative_reward'] = df_run1['reward'].cumsum()\\n\",\n    \"df_run2['cumulative_reward'] = df_run2['reward'].cumsum()\\n\",\n    \"\\n\",\n    \"plt.figure(figsize=(14, 7))\\n\",\n    \"sns.lineplot(data=df_run1, x='episode', y='cumulative_reward', label='Run 1 Cumulative Reward')\\n\",\n    \"sns.lineplot(data=df_run2, x='episode', y='cumulative_reward', label='Run 2 Cumulative Reward')\\n\",\n    \"\\n\",\n    \"plt."
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}